{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UFC Match Predictor (XGBoost & NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "First, we import all the necessary libraries. \n",
    "\n",
    "- `pandas`: For data manipulation and loading the CSV.\n",
    "- `xgboost`: To use the XGBClassifier, our machine learning model.\n",
    "- `matplotlib`: For plotting our evaluation metrics.\n",
    "- `%matplotlib inline`: A Jupyter magic command to make plots appear directly in the notebook.\n",
    "- `sklearn`: We use scikit-learn for:\n",
    "    - `SimpleImputer`: To handle missing (NaN) values.\n",
    "    - `train_test_split`: To create training and testing datasets.\n",
    "    - `KFold` & `cross_val_score`: To perform robust model evaluation.\n",
    "    - `accuracy_score`, `confusion_matrix`, `log_loss`: For scoring our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, log_loss\n",
    "\n",
    "# Setting to display all columns in pandas DataFrames\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration (EDA)\n",
    "\n",
    "We load the dataset and perform an initial Exploratory Data Analysis (EDA) to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:24:37.545212Z",
     "start_time": "2025-10-21T19:24:37.312284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start Chapter 1: Data Exploration ---\n",
      "First 5 rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": "                             event_name          r_fighter        b_fighter  \\\n0  UFC Fight Night: Ribas vs. Namajunas       Amanda Ribas   Rose Namajunas   \n1  UFC Fight Night: Ribas vs. Namajunas      Karl Williams      Justin Tafa   \n2  UFC Fight Night: Ribas vs. Namajunas   Edmen Shahbazyan        AJ Dobson   \n3  UFC Fight Night: Ribas vs. Namajunas     Payton Talbott  Cameron Saaiman   \n4  UFC Fight Night: Ribas vs. Namajunas  Billy Quarantillo    Youssef Zalal   \n\n  winner       weight_class  is_title_bout gender                method  \\\n0   Blue  Women's Flyweight              0  Women  Decision - Unanimous   \n1    Red        Heavyweight              0    Men  Decision - Unanimous   \n2    Red       Middleweight              0    Men                KO/TKO   \n3    Red       Bantamweight              0    Men                KO/TKO   \n4   Blue      Featherweight              0    Men            Submission   \n\n   finish_round  total_rounds  time_sec        referee  r_kd  r_sig_str  \\\n0             5           5.0       300   Jason Herzog     0         83   \n1             3           3.0       300      Herb Dean     0         40   \n2             1           3.0       273     Mark Smith     1         27   \n3             2           3.0        21  Chris Tognoni     1         79   \n4             2           3.0       110      Herb Dean     0         10   \n\n   r_sig_str_att  r_sig_str_acc  r_str  r_str_att  r_str_acc  r_td  r_td_att  \\\n0            270           0.30    137        342       0.40     4         8   \n1             61           0.65    123        176       0.70     7        12   \n2             43           0.62     31         51       0.61     1         3   \n3            129           0.61     83        135       0.61     0         1   \n4             50           0.20     26         74       0.35     0         0   \n\n   r_td_acc  r_sub_att  r_rev  r_ctrl_sec  r_wins_total  r_losses_total  \\\n0      0.50          0      0          98            12               5   \n1      0.58          1      0         631            10               1   \n2      0.33          0      0          49            13               4   \n3      0.00          0      0          30             8               0   \n4      0.00          0      0          21            18               6   \n\n   r_age  r_height  r_weight  r_reach  r_stance  r_SLpM_total  r_SApM_total  \\\n0   30.0    160.02     56.70   167.64  Orthodox          4.63          3.40   \n1   34.0    190.50    106.59   200.66  Orthodox          2.87          1.70   \n2   26.0    187.96     83.91   190.50  Orthodox          3.60          4.09   \n3   25.0    177.80     61.23   177.80    Switch          8.05          3.58   \n4   35.0    177.80     65.77   177.80  Orthodox          7.36          5.57   \n\n   r_sig_str_acc_total  r_td_acc_total  r_str_def_total  r_td_def_total  \\\n0                 0.40            0.51             0.61            0.85   \n1                 0.52            0.50             0.60            1.00   \n2                 0.52            0.38             0.45            0.63   \n3                 0.54            0.00             0.51            0.90   \n4                 0.56            0.23             0.43            0.61   \n\n   r_sub_avg  r_td_avg  b_kd  b_sig_str  b_sig_str_att  b_sig_str_acc  b_str  \\\n0        0.7      2.07     0         93            188           0.49    169   \n1        0.2      4.75     0         21             43           0.48     27   \n2        0.6      2.24     0         15             46           0.32     20   \n3        0.5      0.00     0         31             64           0.48     33   \n4        1.1      1.24     0         33             56           0.58     37   \n\n   b_str_att  b_str_acc  b_td  b_td_att  b_td_acc  b_sub_att  b_rev  \\\n0        281       0.60     1         3      0.33          0      1   \n1         50       0.54     0         0      0.00          0      0   \n2         54       0.37     0         0      0.00          0      1   \n3         66       0.50     0         3      0.00          0      0   \n4         62       0.60     2         2      1.00          2      0   \n\n   b_ctrl_sec  b_wins_total  b_losses_total  b_age  b_height  b_weight  \\\n0         419            13               6   31.0    165.10     56.70   \n1           0             7               4   30.0    182.88    119.75   \n2          75             7               3   32.0    185.42     83.91   \n3           0             9               2   23.0    172.72     61.23   \n4         175            14               5   27.0    177.80     65.77   \n\n   b_reach  b_stance  b_SLpM_total  b_SApM_total  b_sig_str_acc_total  \\\n0   165.10  Orthodox          3.69          3.51                 0.41   \n1   187.96  Southpaw          4.09          5.02                 0.54   \n2   193.04  Orthodox          4.29          5.31                 0.46   \n3   170.18  Southpaw          5.32          4.18                 0.46   \n4   182.88    Switch          2.88          1.73                 0.49   \n\n   b_td_acc_total  b_str_def_total  b_td_def_total  b_sub_avg  b_td_avg  \\\n0            0.47             0.63            0.59        0.5      1.38   \n1            0.00             0.47            0.50        0.0      0.00   \n2            0.75             0.46            0.65        0.3      1.67   \n3            0.28             0.51            0.47        0.7      0.91   \n4            0.34             0.65            0.60        1.3      2.28   \n\n   kd_diff  sig_str_diff  sig_str_att_diff  sig_str_acc_diff  str_diff  \\\n0        0           -10                82             -0.19       -32   \n1        0            19                18              0.17        96   \n2        1            12                -3              0.30        11   \n3        1            48                65              0.13        50   \n4        0           -23                -6             -0.38       -11   \n\n   str_att_diff  str_acc_diff  td_diff  td_att_diff  td_acc_diff  \\\n0            61         -0.20        3            5         0.17   \n1           126          0.16        7           12         0.58   \n2            -3          0.24        1            3         0.33   \n3            69          0.11        0           -2         0.00   \n4            12         -0.25       -2           -2        -1.00   \n\n   sub_att_diff  rev_diff  ctrl_sec_diff  wins_total_diff  losses_total_diff  \\\n0             0        -1           -321               -1                 -1   \n1             1         0            631                3                 -3   \n2             0        -1            -26                6                  1   \n3             0         0             30               -1                 -2   \n4            -2         0           -154                4                  1   \n\n   age_diff  height_diff  weight_diff  reach_diff  SLpM_total_diff  \\\n0      -1.0        -5.08         0.00        2.54             0.94   \n1       4.0         7.62       -13.16       12.70            -1.22   \n2      -6.0         2.54         0.00       -2.54            -0.69   \n3       2.0         5.08         0.00        7.62             2.73   \n4       8.0         0.00         0.00       -5.08             4.48   \n\n   SApM_total_diff  sig_str_acc_total_diff  td_acc_total_diff  \\\n0            -0.11                   -0.01               0.04   \n1            -3.32                   -0.02               0.50   \n2            -1.22                    0.06              -0.37   \n3            -0.60                    0.08              -0.28   \n4             3.84                    0.07              -0.11   \n\n   str_def_total_diff  td_def_total_diff  sub_avg_diff  td_avg_diff  \n0               -0.02               0.26           0.2         0.69  \n1                0.13               0.50           0.2         4.75  \n2               -0.01              -0.02           0.3         0.57  \n3                0.00               0.43          -0.2        -0.91  \n4               -0.22               0.01          -0.2        -1.04  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_name</th>\n      <th>r_fighter</th>\n      <th>b_fighter</th>\n      <th>winner</th>\n      <th>weight_class</th>\n      <th>is_title_bout</th>\n      <th>gender</th>\n      <th>method</th>\n      <th>finish_round</th>\n      <th>total_rounds</th>\n      <th>time_sec</th>\n      <th>referee</th>\n      <th>r_kd</th>\n      <th>r_sig_str</th>\n      <th>r_sig_str_att</th>\n      <th>r_sig_str_acc</th>\n      <th>r_str</th>\n      <th>r_str_att</th>\n      <th>r_str_acc</th>\n      <th>r_td</th>\n      <th>r_td_att</th>\n      <th>r_td_acc</th>\n      <th>r_sub_att</th>\n      <th>r_rev</th>\n      <th>r_ctrl_sec</th>\n      <th>r_wins_total</th>\n      <th>r_losses_total</th>\n      <th>r_age</th>\n      <th>r_height</th>\n      <th>r_weight</th>\n      <th>r_reach</th>\n      <th>r_stance</th>\n      <th>r_SLpM_total</th>\n      <th>r_SApM_total</th>\n      <th>r_sig_str_acc_total</th>\n      <th>r_td_acc_total</th>\n      <th>r_str_def_total</th>\n      <th>r_td_def_total</th>\n      <th>r_sub_avg</th>\n      <th>r_td_avg</th>\n      <th>b_kd</th>\n      <th>b_sig_str</th>\n      <th>b_sig_str_att</th>\n      <th>b_sig_str_acc</th>\n      <th>b_str</th>\n      <th>b_str_att</th>\n      <th>b_str_acc</th>\n      <th>b_td</th>\n      <th>b_td_att</th>\n      <th>b_td_acc</th>\n      <th>b_sub_att</th>\n      <th>b_rev</th>\n      <th>b_ctrl_sec</th>\n      <th>b_wins_total</th>\n      <th>b_losses_total</th>\n      <th>b_age</th>\n      <th>b_height</th>\n      <th>b_weight</th>\n      <th>b_reach</th>\n      <th>b_stance</th>\n      <th>b_SLpM_total</th>\n      <th>b_SApM_total</th>\n      <th>b_sig_str_acc_total</th>\n      <th>b_td_acc_total</th>\n      <th>b_str_def_total</th>\n      <th>b_td_def_total</th>\n      <th>b_sub_avg</th>\n      <th>b_td_avg</th>\n      <th>kd_diff</th>\n      <th>sig_str_diff</th>\n      <th>sig_str_att_diff</th>\n      <th>sig_str_acc_diff</th>\n      <th>str_diff</th>\n      <th>str_att_diff</th>\n      <th>str_acc_diff</th>\n      <th>td_diff</th>\n      <th>td_att_diff</th>\n      <th>td_acc_diff</th>\n      <th>sub_att_diff</th>\n      <th>rev_diff</th>\n      <th>ctrl_sec_diff</th>\n      <th>wins_total_diff</th>\n      <th>losses_total_diff</th>\n      <th>age_diff</th>\n      <th>height_diff</th>\n      <th>weight_diff</th>\n      <th>reach_diff</th>\n      <th>SLpM_total_diff</th>\n      <th>SApM_total_diff</th>\n      <th>sig_str_acc_total_diff</th>\n      <th>td_acc_total_diff</th>\n      <th>str_def_total_diff</th>\n      <th>td_def_total_diff</th>\n      <th>sub_avg_diff</th>\n      <th>td_avg_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UFC Fight Night: Ribas vs. Namajunas</td>\n      <td>Amanda Ribas</td>\n      <td>Rose Namajunas</td>\n      <td>Blue</td>\n      <td>Women's Flyweight</td>\n      <td>0</td>\n      <td>Women</td>\n      <td>Decision - Unanimous</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>300</td>\n      <td>Jason Herzog</td>\n      <td>0</td>\n      <td>83</td>\n      <td>270</td>\n      <td>0.30</td>\n      <td>137</td>\n      <td>342</td>\n      <td>0.40</td>\n      <td>4</td>\n      <td>8</td>\n      <td>0.50</td>\n      <td>0</td>\n      <td>0</td>\n      <td>98</td>\n      <td>12</td>\n      <td>5</td>\n      <td>30.0</td>\n      <td>160.02</td>\n      <td>56.70</td>\n      <td>167.64</td>\n      <td>Orthodox</td>\n      <td>4.63</td>\n      <td>3.40</td>\n      <td>0.40</td>\n      <td>0.51</td>\n      <td>0.61</td>\n      <td>0.85</td>\n      <td>0.7</td>\n      <td>2.07</td>\n      <td>0</td>\n      <td>93</td>\n      <td>188</td>\n      <td>0.49</td>\n      <td>169</td>\n      <td>281</td>\n      <td>0.60</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.33</td>\n      <td>0</td>\n      <td>1</td>\n      <td>419</td>\n      <td>13</td>\n      <td>6</td>\n      <td>31.0</td>\n      <td>165.10</td>\n      <td>56.70</td>\n      <td>165.10</td>\n      <td>Orthodox</td>\n      <td>3.69</td>\n      <td>3.51</td>\n      <td>0.41</td>\n      <td>0.47</td>\n      <td>0.63</td>\n      <td>0.59</td>\n      <td>0.5</td>\n      <td>1.38</td>\n      <td>0</td>\n      <td>-10</td>\n      <td>82</td>\n      <td>-0.19</td>\n      <td>-32</td>\n      <td>61</td>\n      <td>-0.20</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0.17</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-321</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1.0</td>\n      <td>-5.08</td>\n      <td>0.00</td>\n      <td>2.54</td>\n      <td>0.94</td>\n      <td>-0.11</td>\n      <td>-0.01</td>\n      <td>0.04</td>\n      <td>-0.02</td>\n      <td>0.26</td>\n      <td>0.2</td>\n      <td>0.69</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UFC Fight Night: Ribas vs. Namajunas</td>\n      <td>Karl Williams</td>\n      <td>Justin Tafa</td>\n      <td>Red</td>\n      <td>Heavyweight</td>\n      <td>0</td>\n      <td>Men</td>\n      <td>Decision - Unanimous</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>300</td>\n      <td>Herb Dean</td>\n      <td>0</td>\n      <td>40</td>\n      <td>61</td>\n      <td>0.65</td>\n      <td>123</td>\n      <td>176</td>\n      <td>0.70</td>\n      <td>7</td>\n      <td>12</td>\n      <td>0.58</td>\n      <td>1</td>\n      <td>0</td>\n      <td>631</td>\n      <td>10</td>\n      <td>1</td>\n      <td>34.0</td>\n      <td>190.50</td>\n      <td>106.59</td>\n      <td>200.66</td>\n      <td>Orthodox</td>\n      <td>2.87</td>\n      <td>1.70</td>\n      <td>0.52</td>\n      <td>0.50</td>\n      <td>0.60</td>\n      <td>1.00</td>\n      <td>0.2</td>\n      <td>4.75</td>\n      <td>0</td>\n      <td>21</td>\n      <td>43</td>\n      <td>0.48</td>\n      <td>27</td>\n      <td>50</td>\n      <td>0.54</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>4</td>\n      <td>30.0</td>\n      <td>182.88</td>\n      <td>119.75</td>\n      <td>187.96</td>\n      <td>Southpaw</td>\n      <td>4.09</td>\n      <td>5.02</td>\n      <td>0.54</td>\n      <td>0.00</td>\n      <td>0.47</td>\n      <td>0.50</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>19</td>\n      <td>18</td>\n      <td>0.17</td>\n      <td>96</td>\n      <td>126</td>\n      <td>0.16</td>\n      <td>7</td>\n      <td>12</td>\n      <td>0.58</td>\n      <td>1</td>\n      <td>0</td>\n      <td>631</td>\n      <td>3</td>\n      <td>-3</td>\n      <td>4.0</td>\n      <td>7.62</td>\n      <td>-13.16</td>\n      <td>12.70</td>\n      <td>-1.22</td>\n      <td>-3.32</td>\n      <td>-0.02</td>\n      <td>0.50</td>\n      <td>0.13</td>\n      <td>0.50</td>\n      <td>0.2</td>\n      <td>4.75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UFC Fight Night: Ribas vs. Namajunas</td>\n      <td>Edmen Shahbazyan</td>\n      <td>AJ Dobson</td>\n      <td>Red</td>\n      <td>Middleweight</td>\n      <td>0</td>\n      <td>Men</td>\n      <td>KO/TKO</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>273</td>\n      <td>Mark Smith</td>\n      <td>1</td>\n      <td>27</td>\n      <td>43</td>\n      <td>0.62</td>\n      <td>31</td>\n      <td>51</td>\n      <td>0.61</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.33</td>\n      <td>0</td>\n      <td>0</td>\n      <td>49</td>\n      <td>13</td>\n      <td>4</td>\n      <td>26.0</td>\n      <td>187.96</td>\n      <td>83.91</td>\n      <td>190.50</td>\n      <td>Orthodox</td>\n      <td>3.60</td>\n      <td>4.09</td>\n      <td>0.52</td>\n      <td>0.38</td>\n      <td>0.45</td>\n      <td>0.63</td>\n      <td>0.6</td>\n      <td>2.24</td>\n      <td>0</td>\n      <td>15</td>\n      <td>46</td>\n      <td>0.32</td>\n      <td>20</td>\n      <td>54</td>\n      <td>0.37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>1</td>\n      <td>75</td>\n      <td>7</td>\n      <td>3</td>\n      <td>32.0</td>\n      <td>185.42</td>\n      <td>83.91</td>\n      <td>193.04</td>\n      <td>Orthodox</td>\n      <td>4.29</td>\n      <td>5.31</td>\n      <td>0.46</td>\n      <td>0.75</td>\n      <td>0.46</td>\n      <td>0.65</td>\n      <td>0.3</td>\n      <td>1.67</td>\n      <td>1</td>\n      <td>12</td>\n      <td>-3</td>\n      <td>0.30</td>\n      <td>11</td>\n      <td>-3</td>\n      <td>0.24</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.33</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-26</td>\n      <td>6</td>\n      <td>1</td>\n      <td>-6.0</td>\n      <td>2.54</td>\n      <td>0.00</td>\n      <td>-2.54</td>\n      <td>-0.69</td>\n      <td>-1.22</td>\n      <td>0.06</td>\n      <td>-0.37</td>\n      <td>-0.01</td>\n      <td>-0.02</td>\n      <td>0.3</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UFC Fight Night: Ribas vs. Namajunas</td>\n      <td>Payton Talbott</td>\n      <td>Cameron Saaiman</td>\n      <td>Red</td>\n      <td>Bantamweight</td>\n      <td>0</td>\n      <td>Men</td>\n      <td>KO/TKO</td>\n      <td>2</td>\n      <td>3.0</td>\n      <td>21</td>\n      <td>Chris Tognoni</td>\n      <td>1</td>\n      <td>79</td>\n      <td>129</td>\n      <td>0.61</td>\n      <td>83</td>\n      <td>135</td>\n      <td>0.61</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>8</td>\n      <td>0</td>\n      <td>25.0</td>\n      <td>177.80</td>\n      <td>61.23</td>\n      <td>177.80</td>\n      <td>Switch</td>\n      <td>8.05</td>\n      <td>3.58</td>\n      <td>0.54</td>\n      <td>0.00</td>\n      <td>0.51</td>\n      <td>0.90</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>31</td>\n      <td>64</td>\n      <td>0.48</td>\n      <td>33</td>\n      <td>66</td>\n      <td>0.50</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2</td>\n      <td>23.0</td>\n      <td>172.72</td>\n      <td>61.23</td>\n      <td>170.18</td>\n      <td>Southpaw</td>\n      <td>5.32</td>\n      <td>4.18</td>\n      <td>0.46</td>\n      <td>0.28</td>\n      <td>0.51</td>\n      <td>0.47</td>\n      <td>0.7</td>\n      <td>0.91</td>\n      <td>1</td>\n      <td>48</td>\n      <td>65</td>\n      <td>0.13</td>\n      <td>50</td>\n      <td>69</td>\n      <td>0.11</td>\n      <td>0</td>\n      <td>-2</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>-1</td>\n      <td>-2</td>\n      <td>2.0</td>\n      <td>5.08</td>\n      <td>0.00</td>\n      <td>7.62</td>\n      <td>2.73</td>\n      <td>-0.60</td>\n      <td>0.08</td>\n      <td>-0.28</td>\n      <td>0.00</td>\n      <td>0.43</td>\n      <td>-0.2</td>\n      <td>-0.91</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UFC Fight Night: Ribas vs. Namajunas</td>\n      <td>Billy Quarantillo</td>\n      <td>Youssef Zalal</td>\n      <td>Blue</td>\n      <td>Featherweight</td>\n      <td>0</td>\n      <td>Men</td>\n      <td>Submission</td>\n      <td>2</td>\n      <td>3.0</td>\n      <td>110</td>\n      <td>Herb Dean</td>\n      <td>0</td>\n      <td>10</td>\n      <td>50</td>\n      <td>0.20</td>\n      <td>26</td>\n      <td>74</td>\n      <td>0.35</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>18</td>\n      <td>6</td>\n      <td>35.0</td>\n      <td>177.80</td>\n      <td>65.77</td>\n      <td>177.80</td>\n      <td>Orthodox</td>\n      <td>7.36</td>\n      <td>5.57</td>\n      <td>0.56</td>\n      <td>0.23</td>\n      <td>0.43</td>\n      <td>0.61</td>\n      <td>1.1</td>\n      <td>1.24</td>\n      <td>0</td>\n      <td>33</td>\n      <td>56</td>\n      <td>0.58</td>\n      <td>37</td>\n      <td>62</td>\n      <td>0.60</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>175</td>\n      <td>14</td>\n      <td>5</td>\n      <td>27.0</td>\n      <td>177.80</td>\n      <td>65.77</td>\n      <td>182.88</td>\n      <td>Switch</td>\n      <td>2.88</td>\n      <td>1.73</td>\n      <td>0.49</td>\n      <td>0.34</td>\n      <td>0.65</td>\n      <td>0.60</td>\n      <td>1.3</td>\n      <td>2.28</td>\n      <td>0</td>\n      <td>-23</td>\n      <td>-6</td>\n      <td>-0.38</td>\n      <td>-11</td>\n      <td>12</td>\n      <td>-0.25</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-1.00</td>\n      <td>-2</td>\n      <td>0</td>\n      <td>-154</td>\n      <td>4</td>\n      <td>1</td>\n      <td>8.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>-5.08</td>\n      <td>4.48</td>\n      <td>3.84</td>\n      <td>0.07</td>\n      <td>-0.11</td>\n      <td>-0.22</td>\n      <td>0.01</td>\n      <td>-0.2</td>\n      <td>-1.04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--- Start Chapter 1: Data Exploration ---\")\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "df = pd.read_csv('data/large_dataset.csv')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Summary\n",
    "\n",
    "We use `.info()` to get a technical summary, showing data types (like `float64`, `object`) and the count of non-null values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:25:20.770865Z",
     "start_time": "2025-10-21T19:25:20.726884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technical information about the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7439 entries, 0 to 7438\n",
      "Data columns (total 95 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   event_name              7439 non-null   object \n",
      " 1   r_fighter               7439 non-null   object \n",
      " 2   b_fighter               7439 non-null   object \n",
      " 3   winner                  7439 non-null   object \n",
      " 4   weight_class            7439 non-null   object \n",
      " 5   is_title_bout           7439 non-null   int64  \n",
      " 6   gender                  7439 non-null   object \n",
      " 7   method                  7439 non-null   object \n",
      " 8   finish_round            7439 non-null   int64  \n",
      " 9   total_rounds            7408 non-null   float64\n",
      " 10  time_sec                7439 non-null   int64  \n",
      " 11  referee                 7407 non-null   object \n",
      " 12  r_kd                    7439 non-null   int64  \n",
      " 13  r_sig_str               7439 non-null   int64  \n",
      " 14  r_sig_str_att           7439 non-null   int64  \n",
      " 15  r_sig_str_acc           7439 non-null   float64\n",
      " 16  r_str                   7439 non-null   int64  \n",
      " 17  r_str_att               7439 non-null   int64  \n",
      " 18  r_str_acc               7439 non-null   float64\n",
      " 19  r_td                    7439 non-null   int64  \n",
      " 20  r_td_att                7439 non-null   int64  \n",
      " 21  r_td_acc                7439 non-null   float64\n",
      " 22  r_sub_att               7439 non-null   int64  \n",
      " 23  r_rev                   7439 non-null   int64  \n",
      " 24  r_ctrl_sec              7439 non-null   int64  \n",
      " 25  r_wins_total            7439 non-null   int64  \n",
      " 26  r_losses_total          7439 non-null   int64  \n",
      " 27  r_age                   7363 non-null   float64\n",
      " 28  r_height                7439 non-null   float64\n",
      " 29  r_weight                7439 non-null   float64\n",
      " 30  r_reach                 7027 non-null   float64\n",
      " 31  r_stance                7413 non-null   object \n",
      " 32  r_SLpM_total            7439 non-null   float64\n",
      " 33  r_SApM_total            7439 non-null   float64\n",
      " 34  r_sig_str_acc_total     7439 non-null   float64\n",
      " 35  r_td_acc_total          7439 non-null   float64\n",
      " 36  r_str_def_total         7439 non-null   float64\n",
      " 37  r_td_def_total          7439 non-null   float64\n",
      " 38  r_sub_avg               7439 non-null   float64\n",
      " 39  r_td_avg                7439 non-null   float64\n",
      " 40  b_kd                    7439 non-null   int64  \n",
      " 41  b_sig_str               7439 non-null   int64  \n",
      " 42  b_sig_str_att           7439 non-null   int64  \n",
      " 43  b_sig_str_acc           7439 non-null   float64\n",
      " 44  b_str                   7439 non-null   int64  \n",
      " 45  b_str_att               7439 non-null   int64  \n",
      " 46  b_str_acc               7439 non-null   float64\n",
      " 47  b_td                    7439 non-null   int64  \n",
      " 48  b_td_att                7439 non-null   int64  \n",
      " 49  b_td_acc                7439 non-null   float64\n",
      " 50  b_sub_att               7439 non-null   int64  \n",
      " 51  b_rev                   7439 non-null   int64  \n",
      " 52  b_ctrl_sec              7439 non-null   int64  \n",
      " 53  b_wins_total            7439 non-null   int64  \n",
      " 54  b_losses_total          7439 non-null   int64  \n",
      " 55  b_age                   7249 non-null   float64\n",
      " 56  b_height                7439 non-null   float64\n",
      " 57  b_weight                7439 non-null   float64\n",
      " 58  b_reach                 6551 non-null   float64\n",
      " 59  b_stance                7371 non-null   object \n",
      " 60  b_SLpM_total            7439 non-null   float64\n",
      " 61  b_SApM_total            7439 non-null   float64\n",
      " 62  b_sig_str_acc_total     7439 non-null   float64\n",
      " 63  b_td_acc_total          7439 non-null   float64\n",
      " 64  b_str_def_total         7439 non-null   float64\n",
      " 65  b_td_def_total          7439 non-null   float64\n",
      " 66  b_sub_avg               7439 non-null   float64\n",
      " 67  b_td_avg                7439 non-null   float64\n",
      " 68  kd_diff                 7439 non-null   int64  \n",
      " 69  sig_str_diff            7439 non-null   int64  \n",
      " 70  sig_str_att_diff        7439 non-null   int64  \n",
      " 71  sig_str_acc_diff        7439 non-null   float64\n",
      " 72  str_diff                7439 non-null   int64  \n",
      " 73  str_att_diff            7439 non-null   int64  \n",
      " 74  str_acc_diff            7439 non-null   float64\n",
      " 75  td_diff                 7439 non-null   int64  \n",
      " 76  td_att_diff             7439 non-null   int64  \n",
      " 77  td_acc_diff             7439 non-null   float64\n",
      " 78  sub_att_diff            7439 non-null   int64  \n",
      " 79  rev_diff                7439 non-null   int64  \n",
      " 80  ctrl_sec_diff           7439 non-null   int64  \n",
      " 81  wins_total_diff         7439 non-null   int64  \n",
      " 82  losses_total_diff       7439 non-null   int64  \n",
      " 83  age_diff                7226 non-null   float64\n",
      " 84  height_diff             7439 non-null   float64\n",
      " 85  weight_diff             7439 non-null   float64\n",
      " 86  reach_diff              6401 non-null   float64\n",
      " 87  SLpM_total_diff         7439 non-null   float64\n",
      " 88  SApM_total_diff         7439 non-null   float64\n",
      " 89  sig_str_acc_total_diff  7439 non-null   float64\n",
      " 90  td_acc_total_diff       7439 non-null   float64\n",
      " 91  str_def_total_diff      7439 non-null   float64\n",
      " 92  td_def_total_diff       7439 non-null   float64\n",
      " 93  sub_avg_diff            7439 non-null   float64\n",
      " 94  td_avg_diff             7439 non-null   float64\n",
      "dtypes: float64(46), int64(39), object(10)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTechnical information about the dataset:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary\n",
    "\n",
    "We use `.describe()` to get a statistical summary (like mean, median, min, max) for all numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:26:37.535939Z",
     "start_time": "2025-10-21T19:26:37.362212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical summary of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": "       is_title_bout  finish_round  total_rounds     time_sec         r_kd  \\\ncount    7439.000000   7439.000000   7408.000000  7439.000000  7439.000000   \nmean        0.055787      2.336336      3.128915   227.016669     0.249227   \nstd         0.229525      1.015243      0.652739    98.169665     0.524210   \nmin         0.000000      1.000000      1.000000     5.000000     0.000000   \n25%         0.000000      1.000000      3.000000   149.000000     0.000000   \n50%         0.000000      3.000000      3.000000   287.000000     0.000000   \n75%         0.000000      3.000000      3.000000   300.000000     0.000000   \nmax         1.000000      5.000000      5.000000  1080.000000     5.000000   \n\n         r_sig_str  r_sig_str_att  r_sig_str_acc        r_str    r_str_att  \\\ncount  7439.000000    7439.000000    7439.000000  7439.000000  7439.000000   \nmean     38.361204      83.786262       0.475335    58.199892   106.374916   \nstd      32.871278      71.381806       0.165935    46.057503    79.812210   \nmin       0.000000       0.000000       0.000000     0.000000     0.000000   \n25%      14.000000      29.000000       0.370000    22.000000    40.000000   \n50%      31.000000      66.000000       0.470000    50.000000    94.000000   \n75%      54.000000     120.000000       0.570000    83.000000   156.000000   \nmax     445.000000     744.000000       1.000000   447.000000   746.000000   \n\n         r_str_acc         r_td     r_td_att     r_td_acc    r_sub_att  \\\ncount  7439.000000  7439.000000  7439.000000  7439.000000  7439.000000   \nmean      0.555443     1.226509     2.940852     0.311075     0.455706   \nstd       0.176402     1.819250     3.710736     0.372214     0.897335   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.440000     0.000000     0.000000     0.000000     0.000000   \n50%       0.550000     1.000000     1.000000     0.110000     0.000000   \n75%       0.680000     2.000000     4.000000     0.570000     1.000000   \nmax       1.000000    21.000000    27.000000     1.000000    10.000000   \n\n             r_rev   r_ctrl_sec  r_wins_total  r_losses_total        r_age  \\\ncount  7439.000000  7439.000000   7439.000000     7439.000000  7363.000000   \nmean      0.134427   152.120850     19.187928        7.940315    38.316855   \nstd       0.427222   192.097306      9.474207        4.641395     6.426288   \nmin       0.000000     0.000000      0.000000        0.000000    19.000000   \n25%       0.000000     7.000000     13.000000        5.000000    34.000000   \n50%       0.000000    72.000000     18.000000        7.000000    38.000000   \n75%       0.000000   230.000000     24.000000       11.000000    42.000000   \nmax       6.000000  1342.000000    253.000000       53.000000    65.000000   \n\n          r_height     r_weight      r_reach  r_SLpM_total  r_SApM_total  \\\ncount  7439.000000  7439.000000  7027.000000   7439.000000   7439.000000   \nmean    178.620831    76.488396   183.212546      3.411758      3.282863   \nstd       8.969921    16.428678    10.842633      1.332992      1.222434   \nmin     152.400000    52.160000   147.320000      0.000000      0.000000   \n25%     172.720000    65.770000   177.800000      2.570000      2.510000   \n50%     177.800000    77.110000   182.880000      3.330000      3.180000   \n75%     185.420000    83.910000   190.500000      4.205000      3.945000   \nmax     210.820000   156.490000   213.360000     23.330000     15.480000   \n\n       r_sig_str_acc_total  r_td_acc_total  r_str_def_total  r_td_def_total  \\\ncount          7439.000000     7439.000000      7439.000000     7439.000000   \nmean              0.441424        0.388321         0.543109        0.602820   \nstd               0.092338        0.186749         0.097713        0.205218   \nmin               0.000000        0.000000         0.000000        0.000000   \n25%               0.400000        0.290000         0.510000        0.500000   \n50%               0.450000        0.390000         0.560000        0.630000   \n75%               0.500000        0.500000         0.600000        0.740000   \nmax               0.830000        1.000000         0.840000        1.000000   \n\n         r_sub_avg     r_td_avg         b_kd    b_sig_str  b_sig_str_att  \\\ncount  7439.000000  7439.000000  7439.000000  7439.000000    7439.000000   \nmean      0.645194     1.598066     0.181342    33.499933      78.222476   \nstd       0.757643     1.272004     0.455962    30.759088      68.828665   \nmin       0.000000     0.000000     0.000000     0.000000       0.000000   \n25%       0.100000     0.620000     0.000000    10.000000      24.000000   \n50%       0.500000     1.330000     0.000000    25.000000      60.000000   \n75%       0.900000     2.290000     0.000000    48.000000     115.000000   \nmax      13.800000    11.110000     4.000000   241.000000     510.000000   \n\n       b_sig_str_acc        b_str    b_str_att    b_str_acc         b_td  \\\ncount    7439.000000  7439.000000  7439.000000  7439.000000  7439.000000   \nmean        0.429845    49.416992    96.382309     0.515155     0.892996   \nstd         0.174063    41.376059    76.205423     0.186202     1.521771   \nmin         0.000000     0.000000     0.000000     0.000000     0.000000   \n25%         0.330000    16.000000    33.000000     0.400000     0.000000   \n50%         0.430000    41.000000    83.000000     0.520000     0.000000   \n75%         0.530000    72.000000   143.000000     0.640000     1.000000   \nmax         1.000000   336.000000   556.000000     1.000000    12.000000   \n\n          b_td_att     b_td_acc    b_sub_att        b_rev   b_ctrl_sec  \\\ncount  7439.000000  7439.000000  7439.000000  7439.000000  7439.000000   \nmean      2.652507     0.226211     0.325447     0.133082   109.204732   \nstd       3.704621     0.334190     0.765368     0.413578   151.635875   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     2.000000   \n50%       1.000000     0.000000     0.000000     0.000000    42.000000   \n75%       4.000000     0.400000     0.000000     0.000000   159.500000   \nmax      49.000000     1.000000     7.000000     4.000000  1193.000000   \n\n       b_wins_total  b_losses_total        b_age     b_height     b_weight  \\\ncount   7439.000000     7439.000000  7249.000000  7439.000000  7439.000000   \nmean      17.169109        7.399113    37.718996   178.574736    76.317396   \nstd        9.235377        4.471024     6.415852     8.885385    16.700749   \nmin        0.000000        0.000000    21.000000   152.400000    52.160000   \n25%       11.000000        4.000000    33.000000   172.720000    65.770000   \n50%       16.000000        7.000000    37.000000   177.800000    70.310000   \n75%       21.000000       10.000000    42.000000   185.420000    83.910000   \nmax      253.000000       53.000000    81.000000   210.820000   349.270000   \n\n           b_reach  b_SLpM_total  b_SApM_total  b_sig_str_acc_total  \\\ncount  6551.000000   7439.000000   7439.000000          7439.000000   \nmean    182.819127      3.269164      3.454506             0.429314   \nstd      10.660761      1.451367      1.549403             0.110161   \nmin     147.320000      0.000000      0.000000             0.000000   \n25%     175.260000      2.340000      2.600000             0.390000   \n50%     182.880000      3.250000      3.290000             0.440000   \n75%     190.500000      4.110000      4.180000             0.490000   \nmax     213.360000     11.030000     42.000000             1.000000   \n\n       b_td_acc_total  b_str_def_total  b_td_def_total    b_sub_avg  \\\ncount     7439.000000      7439.000000     7439.000000  7439.000000   \nmean         0.360157         0.522201        0.565307     0.599543   \nstd          0.213430         0.116185        0.239035     0.807908   \nmin          0.000000         0.000000        0.000000     0.000000   \n25%          0.250000         0.490000        0.450000     0.000000   \n50%          0.360000         0.540000        0.610000     0.400000   \n75%          0.480000         0.590000        0.720000     0.800000   \nmax          1.000000         1.000000        1.000000    16.400000   \n\n          b_td_avg      kd_diff  sig_str_diff  sig_str_att_diff  \\\ncount  7439.000000  7439.000000   7439.000000       7439.000000   \nmean      1.463578     0.067885      4.861272          5.563785   \nstd       1.310877     0.731381     26.498537         42.933081   \nmin       0.000000    -4.000000   -157.000000       -318.000000   \n25%       0.490000     0.000000     -8.000000        -14.000000   \n50%       1.160000     0.000000      4.000000          4.000000   \n75%       2.120000     0.000000     18.000000         26.000000   \nmax      13.950000     5.000000    312.000000        461.000000   \n\n       sig_str_acc_diff     str_diff  str_att_diff  str_acc_diff      td_diff  \\\ncount       7439.000000  7439.000000   7439.000000   7439.000000  7439.000000   \nmean           0.045490     8.782901      9.992607      0.040288     0.333513   \nstd            0.240379    45.842688     61.554186      0.229646     2.509234   \nmin           -1.000000  -276.000000   -339.000000     -1.000000   -12.000000   \n25%           -0.090000   -12.000000    -19.000000     -0.090000    -1.000000   \n50%            0.040000     6.000000      6.000000      0.030000     0.000000   \n75%            0.170000    29.000000     38.000000      0.170000     1.000000   \nmax            1.000000   315.000000    462.000000      1.000000    20.000000   \n\n       td_att_diff  td_acc_diff  sub_att_diff     rev_diff  ctrl_sec_diff  \\\ncount  7439.000000  7439.000000   7439.000000  7439.000000    7439.000000   \nmean      0.288345     0.084865      0.130259     0.001344      42.916118   \nstd       5.508415     0.524888      1.154435     0.482552     248.351021   \nmin     -44.000000    -1.000000     -7.000000    -3.000000   -1164.000000   \n25%      -2.000000    -0.200000      0.000000     0.000000     -53.000000   \n50%       0.000000     0.000000      0.000000     0.000000       5.000000   \n75%       3.000000     0.500000      0.000000     0.000000     144.000000   \nmax      26.000000     1.000000     10.000000     5.000000    1301.000000   \n\n       wins_total_diff  losses_total_diff     age_diff  height_diff  \\\ncount       7439.00000        7439.000000  7226.000000  7439.000000   \nmean           2.01882           0.541202     0.388043     0.046095   \nstd           11.60913           5.727700     5.223528     6.497921   \nmin         -241.00000         -47.000000   -24.000000   -30.480000   \n25%           -4.00000          -3.000000    -3.000000    -5.080000   \n50%            2.00000           0.000000     0.000000     0.000000   \n75%            8.00000           4.000000     4.000000     5.080000   \nmax          249.00000          46.000000    17.000000    33.020000   \n\n       weight_diff   reach_diff  SLpM_total_diff  SApM_total_diff  \\\ncount  7439.000000  6401.000000      7439.000000      7439.000000   \nmean      0.171000     0.190073         0.142594        -0.171643   \nstd       6.774199     8.252628         1.585610         1.691358   \nmin    -258.550000   -27.940000        -8.990000       -39.490000   \n25%       0.000000    -5.080000        -0.860000        -1.040000   \n50%       0.000000     0.000000         0.130000        -0.120000   \n75%       0.000000     5.080000         1.160000         0.790000   \nmax      52.160000    33.020000        18.780000        12.640000   \n\n       sig_str_acc_total_diff  td_acc_total_diff  str_def_total_diff  \\\ncount             7439.000000        7439.000000         7439.000000   \nmean                 0.012109           0.028164            0.020909   \nstd                  0.119919           0.276307            0.113455   \nmin                 -0.700000          -1.000000           -0.580000   \n25%                 -0.060000          -0.130000           -0.040000   \n50%                  0.010000           0.020000            0.010000   \n75%                  0.080000           0.190000            0.080000   \nmax                  0.830000           1.000000            0.720000   \n\n       td_def_total_diff  sub_avg_diff  td_avg_diff  \ncount        7439.000000   7439.000000  7439.000000  \nmean            0.037513      0.045651     0.134487  \nstd             0.292107      1.052065     1.781598  \nmin            -1.000000    -15.100000   -11.770000  \n25%            -0.140000     -0.400000    -0.870000  \n50%             0.020000      0.000000     0.090000  \n75%             0.210000      0.500000     1.160000  \nmax             1.000000     13.800000    11.110000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_title_bout</th>\n      <th>finish_round</th>\n      <th>total_rounds</th>\n      <th>time_sec</th>\n      <th>r_kd</th>\n      <th>r_sig_str</th>\n      <th>r_sig_str_att</th>\n      <th>r_sig_str_acc</th>\n      <th>r_str</th>\n      <th>r_str_att</th>\n      <th>r_str_acc</th>\n      <th>r_td</th>\n      <th>r_td_att</th>\n      <th>r_td_acc</th>\n      <th>r_sub_att</th>\n      <th>r_rev</th>\n      <th>r_ctrl_sec</th>\n      <th>r_wins_total</th>\n      <th>r_losses_total</th>\n      <th>r_age</th>\n      <th>r_height</th>\n      <th>r_weight</th>\n      <th>r_reach</th>\n      <th>r_SLpM_total</th>\n      <th>r_SApM_total</th>\n      <th>r_sig_str_acc_total</th>\n      <th>r_td_acc_total</th>\n      <th>r_str_def_total</th>\n      <th>r_td_def_total</th>\n      <th>r_sub_avg</th>\n      <th>r_td_avg</th>\n      <th>b_kd</th>\n      <th>b_sig_str</th>\n      <th>b_sig_str_att</th>\n      <th>b_sig_str_acc</th>\n      <th>b_str</th>\n      <th>b_str_att</th>\n      <th>b_str_acc</th>\n      <th>b_td</th>\n      <th>b_td_att</th>\n      <th>b_td_acc</th>\n      <th>b_sub_att</th>\n      <th>b_rev</th>\n      <th>b_ctrl_sec</th>\n      <th>b_wins_total</th>\n      <th>b_losses_total</th>\n      <th>b_age</th>\n      <th>b_height</th>\n      <th>b_weight</th>\n      <th>b_reach</th>\n      <th>b_SLpM_total</th>\n      <th>b_SApM_total</th>\n      <th>b_sig_str_acc_total</th>\n      <th>b_td_acc_total</th>\n      <th>b_str_def_total</th>\n      <th>b_td_def_total</th>\n      <th>b_sub_avg</th>\n      <th>b_td_avg</th>\n      <th>kd_diff</th>\n      <th>sig_str_diff</th>\n      <th>sig_str_att_diff</th>\n      <th>sig_str_acc_diff</th>\n      <th>str_diff</th>\n      <th>str_att_diff</th>\n      <th>str_acc_diff</th>\n      <th>td_diff</th>\n      <th>td_att_diff</th>\n      <th>td_acc_diff</th>\n      <th>sub_att_diff</th>\n      <th>rev_diff</th>\n      <th>ctrl_sec_diff</th>\n      <th>wins_total_diff</th>\n      <th>losses_total_diff</th>\n      <th>age_diff</th>\n      <th>height_diff</th>\n      <th>weight_diff</th>\n      <th>reach_diff</th>\n      <th>SLpM_total_diff</th>\n      <th>SApM_total_diff</th>\n      <th>sig_str_acc_total_diff</th>\n      <th>td_acc_total_diff</th>\n      <th>str_def_total_diff</th>\n      <th>td_def_total_diff</th>\n      <th>sub_avg_diff</th>\n      <th>td_avg_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7408.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7363.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7027.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7249.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>6551.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.00000</td>\n      <td>7439.000000</td>\n      <td>7226.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>6401.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n      <td>7439.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.055787</td>\n      <td>2.336336</td>\n      <td>3.128915</td>\n      <td>227.016669</td>\n      <td>0.249227</td>\n      <td>38.361204</td>\n      <td>83.786262</td>\n      <td>0.475335</td>\n      <td>58.199892</td>\n      <td>106.374916</td>\n      <td>0.555443</td>\n      <td>1.226509</td>\n      <td>2.940852</td>\n      <td>0.311075</td>\n      <td>0.455706</td>\n      <td>0.134427</td>\n      <td>152.120850</td>\n      <td>19.187928</td>\n      <td>7.940315</td>\n      <td>38.316855</td>\n      <td>178.620831</td>\n      <td>76.488396</td>\n      <td>183.212546</td>\n      <td>3.411758</td>\n      <td>3.282863</td>\n      <td>0.441424</td>\n      <td>0.388321</td>\n      <td>0.543109</td>\n      <td>0.602820</td>\n      <td>0.645194</td>\n      <td>1.598066</td>\n      <td>0.181342</td>\n      <td>33.499933</td>\n      <td>78.222476</td>\n      <td>0.429845</td>\n      <td>49.416992</td>\n      <td>96.382309</td>\n      <td>0.515155</td>\n      <td>0.892996</td>\n      <td>2.652507</td>\n      <td>0.226211</td>\n      <td>0.325447</td>\n      <td>0.133082</td>\n      <td>109.204732</td>\n      <td>17.169109</td>\n      <td>7.399113</td>\n      <td>37.718996</td>\n      <td>178.574736</td>\n      <td>76.317396</td>\n      <td>182.819127</td>\n      <td>3.269164</td>\n      <td>3.454506</td>\n      <td>0.429314</td>\n      <td>0.360157</td>\n      <td>0.522201</td>\n      <td>0.565307</td>\n      <td>0.599543</td>\n      <td>1.463578</td>\n      <td>0.067885</td>\n      <td>4.861272</td>\n      <td>5.563785</td>\n      <td>0.045490</td>\n      <td>8.782901</td>\n      <td>9.992607</td>\n      <td>0.040288</td>\n      <td>0.333513</td>\n      <td>0.288345</td>\n      <td>0.084865</td>\n      <td>0.130259</td>\n      <td>0.001344</td>\n      <td>42.916118</td>\n      <td>2.01882</td>\n      <td>0.541202</td>\n      <td>0.388043</td>\n      <td>0.046095</td>\n      <td>0.171000</td>\n      <td>0.190073</td>\n      <td>0.142594</td>\n      <td>-0.171643</td>\n      <td>0.012109</td>\n      <td>0.028164</td>\n      <td>0.020909</td>\n      <td>0.037513</td>\n      <td>0.045651</td>\n      <td>0.134487</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.229525</td>\n      <td>1.015243</td>\n      <td>0.652739</td>\n      <td>98.169665</td>\n      <td>0.524210</td>\n      <td>32.871278</td>\n      <td>71.381806</td>\n      <td>0.165935</td>\n      <td>46.057503</td>\n      <td>79.812210</td>\n      <td>0.176402</td>\n      <td>1.819250</td>\n      <td>3.710736</td>\n      <td>0.372214</td>\n      <td>0.897335</td>\n      <td>0.427222</td>\n      <td>192.097306</td>\n      <td>9.474207</td>\n      <td>4.641395</td>\n      <td>6.426288</td>\n      <td>8.969921</td>\n      <td>16.428678</td>\n      <td>10.842633</td>\n      <td>1.332992</td>\n      <td>1.222434</td>\n      <td>0.092338</td>\n      <td>0.186749</td>\n      <td>0.097713</td>\n      <td>0.205218</td>\n      <td>0.757643</td>\n      <td>1.272004</td>\n      <td>0.455962</td>\n      <td>30.759088</td>\n      <td>68.828665</td>\n      <td>0.174063</td>\n      <td>41.376059</td>\n      <td>76.205423</td>\n      <td>0.186202</td>\n      <td>1.521771</td>\n      <td>3.704621</td>\n      <td>0.334190</td>\n      <td>0.765368</td>\n      <td>0.413578</td>\n      <td>151.635875</td>\n      <td>9.235377</td>\n      <td>4.471024</td>\n      <td>6.415852</td>\n      <td>8.885385</td>\n      <td>16.700749</td>\n      <td>10.660761</td>\n      <td>1.451367</td>\n      <td>1.549403</td>\n      <td>0.110161</td>\n      <td>0.213430</td>\n      <td>0.116185</td>\n      <td>0.239035</td>\n      <td>0.807908</td>\n      <td>1.310877</td>\n      <td>0.731381</td>\n      <td>26.498537</td>\n      <td>42.933081</td>\n      <td>0.240379</td>\n      <td>45.842688</td>\n      <td>61.554186</td>\n      <td>0.229646</td>\n      <td>2.509234</td>\n      <td>5.508415</td>\n      <td>0.524888</td>\n      <td>1.154435</td>\n      <td>0.482552</td>\n      <td>248.351021</td>\n      <td>11.60913</td>\n      <td>5.727700</td>\n      <td>5.223528</td>\n      <td>6.497921</td>\n      <td>6.774199</td>\n      <td>8.252628</td>\n      <td>1.585610</td>\n      <td>1.691358</td>\n      <td>0.119919</td>\n      <td>0.276307</td>\n      <td>0.113455</td>\n      <td>0.292107</td>\n      <td>1.052065</td>\n      <td>1.781598</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>19.000000</td>\n      <td>152.400000</td>\n      <td>52.160000</td>\n      <td>147.320000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>21.000000</td>\n      <td>152.400000</td>\n      <td>52.160000</td>\n      <td>147.320000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-4.000000</td>\n      <td>-157.000000</td>\n      <td>-318.000000</td>\n      <td>-1.000000</td>\n      <td>-276.000000</td>\n      <td>-339.000000</td>\n      <td>-1.000000</td>\n      <td>-12.000000</td>\n      <td>-44.000000</td>\n      <td>-1.000000</td>\n      <td>-7.000000</td>\n      <td>-3.000000</td>\n      <td>-1164.000000</td>\n      <td>-241.00000</td>\n      <td>-47.000000</td>\n      <td>-24.000000</td>\n      <td>-30.480000</td>\n      <td>-258.550000</td>\n      <td>-27.940000</td>\n      <td>-8.990000</td>\n      <td>-39.490000</td>\n      <td>-0.700000</td>\n      <td>-1.000000</td>\n      <td>-0.580000</td>\n      <td>-1.000000</td>\n      <td>-15.100000</td>\n      <td>-11.770000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>149.000000</td>\n      <td>0.000000</td>\n      <td>14.000000</td>\n      <td>29.000000</td>\n      <td>0.370000</td>\n      <td>22.000000</td>\n      <td>40.000000</td>\n      <td>0.440000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.000000</td>\n      <td>13.000000</td>\n      <td>5.000000</td>\n      <td>34.000000</td>\n      <td>172.720000</td>\n      <td>65.770000</td>\n      <td>177.800000</td>\n      <td>2.570000</td>\n      <td>2.510000</td>\n      <td>0.400000</td>\n      <td>0.290000</td>\n      <td>0.510000</td>\n      <td>0.500000</td>\n      <td>0.100000</td>\n      <td>0.620000</td>\n      <td>0.000000</td>\n      <td>10.000000</td>\n      <td>24.000000</td>\n      <td>0.330000</td>\n      <td>16.000000</td>\n      <td>33.000000</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>11.000000</td>\n      <td>4.000000</td>\n      <td>33.000000</td>\n      <td>172.720000</td>\n      <td>65.770000</td>\n      <td>175.260000</td>\n      <td>2.340000</td>\n      <td>2.600000</td>\n      <td>0.390000</td>\n      <td>0.250000</td>\n      <td>0.490000</td>\n      <td>0.450000</td>\n      <td>0.000000</td>\n      <td>0.490000</td>\n      <td>0.000000</td>\n      <td>-8.000000</td>\n      <td>-14.000000</td>\n      <td>-0.090000</td>\n      <td>-12.000000</td>\n      <td>-19.000000</td>\n      <td>-0.090000</td>\n      <td>-1.000000</td>\n      <td>-2.000000</td>\n      <td>-0.200000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-53.000000</td>\n      <td>-4.00000</td>\n      <td>-3.000000</td>\n      <td>-3.000000</td>\n      <td>-5.080000</td>\n      <td>0.000000</td>\n      <td>-5.080000</td>\n      <td>-0.860000</td>\n      <td>-1.040000</td>\n      <td>-0.060000</td>\n      <td>-0.130000</td>\n      <td>-0.040000</td>\n      <td>-0.140000</td>\n      <td>-0.400000</td>\n      <td>-0.870000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>287.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n      <td>66.000000</td>\n      <td>0.470000</td>\n      <td>50.000000</td>\n      <td>94.000000</td>\n      <td>0.550000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.110000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>72.000000</td>\n      <td>18.000000</td>\n      <td>7.000000</td>\n      <td>38.000000</td>\n      <td>177.800000</td>\n      <td>77.110000</td>\n      <td>182.880000</td>\n      <td>3.330000</td>\n      <td>3.180000</td>\n      <td>0.450000</td>\n      <td>0.390000</td>\n      <td>0.560000</td>\n      <td>0.630000</td>\n      <td>0.500000</td>\n      <td>1.330000</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n      <td>60.000000</td>\n      <td>0.430000</td>\n      <td>41.000000</td>\n      <td>83.000000</td>\n      <td>0.520000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>42.000000</td>\n      <td>16.000000</td>\n      <td>7.000000</td>\n      <td>37.000000</td>\n      <td>177.800000</td>\n      <td>70.310000</td>\n      <td>182.880000</td>\n      <td>3.250000</td>\n      <td>3.290000</td>\n      <td>0.440000</td>\n      <td>0.360000</td>\n      <td>0.540000</td>\n      <td>0.610000</td>\n      <td>0.400000</td>\n      <td>1.160000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>0.040000</td>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>0.030000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>2.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.130000</td>\n      <td>-0.120000</td>\n      <td>0.010000</td>\n      <td>0.020000</td>\n      <td>0.010000</td>\n      <td>0.020000</td>\n      <td>0.000000</td>\n      <td>0.090000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>300.000000</td>\n      <td>0.000000</td>\n      <td>54.000000</td>\n      <td>120.000000</td>\n      <td>0.570000</td>\n      <td>83.000000</td>\n      <td>156.000000</td>\n      <td>0.680000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>0.570000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>230.000000</td>\n      <td>24.000000</td>\n      <td>11.000000</td>\n      <td>42.000000</td>\n      <td>185.420000</td>\n      <td>83.910000</td>\n      <td>190.500000</td>\n      <td>4.205000</td>\n      <td>3.945000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.600000</td>\n      <td>0.740000</td>\n      <td>0.900000</td>\n      <td>2.290000</td>\n      <td>0.000000</td>\n      <td>48.000000</td>\n      <td>115.000000</td>\n      <td>0.530000</td>\n      <td>72.000000</td>\n      <td>143.000000</td>\n      <td>0.640000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>159.500000</td>\n      <td>21.000000</td>\n      <td>10.000000</td>\n      <td>42.000000</td>\n      <td>185.420000</td>\n      <td>83.910000</td>\n      <td>190.500000</td>\n      <td>4.110000</td>\n      <td>4.180000</td>\n      <td>0.490000</td>\n      <td>0.480000</td>\n      <td>0.590000</td>\n      <td>0.720000</td>\n      <td>0.800000</td>\n      <td>2.120000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>26.000000</td>\n      <td>0.170000</td>\n      <td>29.000000</td>\n      <td>38.000000</td>\n      <td>0.170000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>144.000000</td>\n      <td>8.00000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>5.080000</td>\n      <td>0.000000</td>\n      <td>5.080000</td>\n      <td>1.160000</td>\n      <td>0.790000</td>\n      <td>0.080000</td>\n      <td>0.190000</td>\n      <td>0.080000</td>\n      <td>0.210000</td>\n      <td>0.500000</td>\n      <td>1.160000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>1080.000000</td>\n      <td>5.000000</td>\n      <td>445.000000</td>\n      <td>744.000000</td>\n      <td>1.000000</td>\n      <td>447.000000</td>\n      <td>746.000000</td>\n      <td>1.000000</td>\n      <td>21.000000</td>\n      <td>27.000000</td>\n      <td>1.000000</td>\n      <td>10.000000</td>\n      <td>6.000000</td>\n      <td>1342.000000</td>\n      <td>253.000000</td>\n      <td>53.000000</td>\n      <td>65.000000</td>\n      <td>210.820000</td>\n      <td>156.490000</td>\n      <td>213.360000</td>\n      <td>23.330000</td>\n      <td>15.480000</td>\n      <td>0.830000</td>\n      <td>1.000000</td>\n      <td>0.840000</td>\n      <td>1.000000</td>\n      <td>13.800000</td>\n      <td>11.110000</td>\n      <td>4.000000</td>\n      <td>241.000000</td>\n      <td>510.000000</td>\n      <td>1.000000</td>\n      <td>336.000000</td>\n      <td>556.000000</td>\n      <td>1.000000</td>\n      <td>12.000000</td>\n      <td>49.000000</td>\n      <td>1.000000</td>\n      <td>7.000000</td>\n      <td>4.000000</td>\n      <td>1193.000000</td>\n      <td>253.000000</td>\n      <td>53.000000</td>\n      <td>81.000000</td>\n      <td>210.820000</td>\n      <td>349.270000</td>\n      <td>213.360000</td>\n      <td>11.030000</td>\n      <td>42.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>16.400000</td>\n      <td>13.950000</td>\n      <td>5.000000</td>\n      <td>312.000000</td>\n      <td>461.000000</td>\n      <td>1.000000</td>\n      <td>315.000000</td>\n      <td>462.000000</td>\n      <td>1.000000</td>\n      <td>20.000000</td>\n      <td>26.000000</td>\n      <td>1.000000</td>\n      <td>10.000000</td>\n      <td>5.000000</td>\n      <td>1301.000000</td>\n      <td>249.00000</td>\n      <td>46.000000</td>\n      <td>17.000000</td>\n      <td>33.020000</td>\n      <td>52.160000</td>\n      <td>33.020000</td>\n      <td>18.780000</td>\n      <td>12.640000</td>\n      <td>0.830000</td>\n      <td>1.000000</td>\n      <td>0.720000</td>\n      <td>1.000000</td>\n      <td>13.800000</td>\n      <td>11.110000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nStatistical summary of the dataset:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Distribution\n",
    "\n",
    "We check the distribution of our target variable, `winner`. This is important to see if our dataset is balanced or imbalanced. An imbalanced dataset (e.g., 90% Red wins, 10% Blue wins) would require special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:29:03.313231Z",
     "start_time": "2025-10-21T19:29:03.163150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in the 'winner' column:\n"
     ]
    },
    {
     "data": {
      "text/plain": "winner\nRed     4876\nBlue    2563\nName: count, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nUnique values in the 'winner' column:\")\n",
    "df['winner'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Preprocessing\n",
    "\n",
    "Now we clean the data and prepare it for the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding\n",
    "\n",
    "Our target variable `winner` is text ('Red', 'Blue'). Machine learning models require numerical input. We will convert 'Red' to `1` and 'Blue' to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:29:17.252988Z",
     "start_time": "2025-10-21T19:29:17.186224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Start Chapter 2: Data Cleaning & Preprocessing ---\n",
      "\n",
      "'winner' column converting to numerical (1 for Red, 0 for Blue)...\n",
      "Conversion complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebube\\AppData\\Local\\Temp\\ipykernel_7512\\1926607315.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['winner'] = df['winner'].replace({'Red': 1, 'Blue': 0})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Start Chapter 2: Data Cleaning & Preprocessing ---\")\n",
    "print(\"\\n'winner' column converting to numerical (1 for Red, 0 for Blue)...\")\n",
    "df['winner'] = df['winner'].replace({'Red': 1, 'Blue': 0})\n",
    "print(\"Conversion complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Features (X) and Target (y)\n",
    "\n",
    "We separate our dataset into two parts:\n",
    "- `y` (the target): The single column we want to predict (`winner`).\n",
    "- `X` (the features): All the other columns we will use to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:29:40.190243Z",
     "start_time": "2025-10-21T19:29:40.153444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separating data into X (features) and y (target)...\n",
      "Separation complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSeparating data into X (features) and y (target)...\")\n",
    "# 'y' is the column we want to predict\n",
    "y = df['winner']\n",
    "# 'X' is everything else\n",
    "X = df.drop(columns=['winner'])\n",
    "print(\"Separation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Dropping\n",
    "\n",
    "We remove columns from `X` that are not useful for prediction. These include:\n",
    "- **Data Leakage:** Columns that would \"cheat\" (e.g., `finish_round`, `method`). These are results of the fight, which we wouldn't know beforehand.\n",
    "- **Per-Minute Stats:** We drop the raw count stats (e.g., `r_sig_str`) and keep the *per-minute* averages (which are already in the dataset, e.g., `r_avg_sig_str_pm`).\n",
    "- **Identifiers:** Text-based IDs like `event_name`, `r_fighter`, and `referee` are not numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:32:24.502900Z",
     "start_time": "2025-10-21T19:32:24.469838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning X (features)...\n",
      "37 columns removed from X.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaning X (features)...\")\n",
    "columns_to_drop_from_X = [\n",
    "    'r_kd', 'r_sig_str', 'r_sig_str_att', 'r_sig_str_acc', 'r_str', 'r_str_att', 'r_str_acc',\n",
    "    'r_td', 'r_td_att', 'r_td_acc', 'r_sub_att', 'r_rev', 'r_ctrl_sec',\n",
    "    'b_kd', 'b_sig_str', 'b_sig_str_att', 'b_sig_str_acc', 'b_str', 'b_str_att', 'b_str_acc',\n",
    "    'b_td', 'b_td_att', 'b_td_acc', 'b_sub_att', 'b_rev', 'b_ctrl_sec',\n",
    "    'finish_round', 'time_sec', 'method',\n",
    "    'event_name', 'r_fighter', 'b_fighter', 'weight_class', 'gender', 'referee', 'r_stance', 'b_stance'\n",
    "]\n",
    "X_clean = X.drop(columns=columns_to_drop_from_X)\n",
    "print(f\"{len(columns_to_drop_from_X)} columns removed from X.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values (Imputation)\n",
    "\n",
    "Some columns might have missing values (NaN). XGBoost can sometimes handle this, but it's cleaner to fill them. We use `SimpleImputer` from scikit-learn to fill all missing values with the `mean` (average) of their respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:32:59.318538Z",
     "start_time": "2025-10-21T19:32:59.152645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Start Chapter 3: Filling Missing Values ---\n",
      "Missing values filled.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Start Chapter 3: Filling Missing Values ---\")\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_final = pd.DataFrame(imputer.fit_transform(X_clean), columns=X_clean.columns)\n",
    "print(\"Missing values filled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation & Training\n",
    "\n",
    "This is where we test our model. We will first get a robust accuracy score using Cross-Validation, and then train a final model to see which features are most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Cross-Validation\n",
    "\n",
    "Instead of a simple train/test split, we use 5-Fold Cross-Validation. This splits the data into 5 parts (\"folds\"). It trains the model 5 times, each time using a different fold as the test set and the other 4 as the training set.\n",
    "\n",
    "This gives a much more reliable estimate of our model's true accuracy on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:36:17.805425Z",
     "start_time": "2025-10-21T19:36:15.802985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Start Chapter 5: Evaluation & Final Training ---\n",
      "\n",
      "Phase 1: Evaluating model performance with 5-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:36:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:36:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:36:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:36:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:36:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each of the 5 Folds: [0.88776882 0.89516129 0.88508065 0.89112903 0.89240081]\n",
      "Average Accuracy: 89.03%\n",
      "Standard Deviation: 0.35%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Start Chapter 5: Evaluation & Final Training ---\")\n",
    "\n",
    "# We initialize our model (the 'blueprint')\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# --- Phase 1: Evaluate with Cross-Validation ---\n",
    "print(\"\\nPhase 1: Evaluating model performance with 5-Fold Cross-Validation...\")\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X_final, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(f\"Scores for each of the 5 Folds: {scores}\")\n",
    "print(f\"Average Accuracy: {scores.mean() * 100:.2f}%\")\n",
    "print(f\"Standard Deviation: {scores.std() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Final Model Training & Feature Importance\n",
    "\n",
    "Now that we know our model's performance (e.g., +-90% accuracy), we train one final model on 100% of the data. \n",
    "\n",
    "We can then inspect this fully-trained model to see which features it found most predictive. This is a key part of machine learning, as it helps us understand *why* the model makes its decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:37:22.796323Z",
     "start_time": "2025-10-21T19:37:22.400170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 2: Training final model on ALL data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:37:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained!\n",
      "\n",
      "Top 10 Most Important Features according to the final model:\n"
     ]
    },
    {
     "data": {
      "text/plain": "              feature  importance\n31       sig_str_diff    0.205078\n30            kd_diff    0.080026\n40       sub_att_diff    0.066758\n34           str_diff    0.057426\n42      ctrl_sec_diff    0.040452\n1        total_rounds    0.023808\n18              b_age    0.023786\n37            td_diff    0.020310\n44  losses_total_diff    0.015801\n43    wins_total_diff    0.015401",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>31</th>\n      <td>sig_str_diff</td>\n      <td>0.205078</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>kd_diff</td>\n      <td>0.080026</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>sub_att_diff</td>\n      <td>0.066758</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>str_diff</td>\n      <td>0.057426</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>ctrl_sec_diff</td>\n      <td>0.040452</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>total_rounds</td>\n      <td>0.023808</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>b_age</td>\n      <td>0.023786</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>td_diff</td>\n      <td>0.020310</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>losses_total_diff</td>\n      <td>0.015801</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>wins_total_diff</td>\n      <td>0.015401</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nPhase 2: Training final model on ALL data...\")\n",
    "# We use the same 'model' object, but now train it on 100% of the data\n",
    "model.fit(X_final, y)\n",
    "print(\"Final model trained!\")\n",
    "\n",
    "# Now we can check the feature importances of this final, fully trained model\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X_final.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features according to the final model:\")\n",
    "feature_importances.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Evaluation\n",
    "\n",
    "Accuracy is good, but not the only metric. For advanced metrics like a Confusion Matrix and Log Loss, we need a dedicated `test` set. We will create a standard 80% training / 20% testing split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "We split the data so we can train the model on one part and test it on another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:38:03.999434Z",
     "start_time": "2025-10-21T19:38:03.971082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Start Chapter 6: Advanced Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Start Chapter 6: Advanced Evaluation ---\")\n",
    "\n",
    "# We need a train/test split for the confusion matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train Model on Training Set\n",
    "\n",
    "We train the model *only* on the `X_train` data and then make predictions on the unseen `X_test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:38:15.135234Z",
     "start_time": "2025-10-21T19:38:14.584060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:38:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "proba_predictions = model.predict_proba(X_test) # Get the probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Confusion Matrix\n",
    "\n",
    "A confusion matrix shows us *what kinds* of mistakes our model is making. It's much more informative than accuracy alone.\n",
    "\n",
    "**How to Read This Matrix:**\n",
    "* **Top-Left (True Negative):** The model predicted 'Blue Wins', and 'Blue' *did* win. (Correct)\n",
    "* **Bottom-Right (True Positive):** The model predicted 'Red Wins', and 'Red' *did* win. (Correct)\n",
    "* **Top-Right (False Positive):** The model predicted 'Red Wins', but 'Blue' *actually* won. (Incorrect)\n",
    "* **Bottom-Left (False Negative):** The model predicted 'Blue Wins', but 'Red' *actually* won. (Incorrect)\n",
    "\n",
    "By looking at the numbers in the incorrect squares, we can see if our model has a bias (e.g., if it's much worse at predicting Blue wins than Red wins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:38:53.303794Z",
     "start_time": "2025-10-21T19:38:53.159069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[433  84]\n",
      " [ 83 888]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)\n",
    "\n",
    "# Visualize the Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Blue Wins', 'Red Wins'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Log Loss (on Test Set)\n",
    "\n",
    "`Log Loss` is a metric that heavily penalizes *confident wrong answers*. \n",
    "\n",
    "- If the model says \"I am 99% sure Red wins\" and Blue wins, it gets a high penalty.\n",
    "- If it says \"I am 51% sure Red wins\" and Blue wins, it gets a small penalty.\n",
    "\n",
    "A lower Log Loss score is better (a perfect score is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:39:00.882695Z",
     "start_time": "2025-10-21T19:39:00.850829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log Loss on the test set: 0.3048\n"
     ]
    }
   ],
   "source": [
    "logloss_score = log_loss(y_test, proba_predictions)\n",
    "print(f\"\\nLog Loss on the test set: {logloss_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Robust Log Loss (with Cross-Validation)\n",
    "\n",
    "Finally, just as we did with accuracy, we can get a more robust `Log Loss` score by using 5-fold cross-validation. This gives us the best estimate of our model's true confidence and penalty-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:39:06.594979Z",
     "start_time": "2025-10-21T19:39:04.455924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Log Loss with 5-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:39:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:39:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:39:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:39:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\ebube\\OneDrive\\Documenten\\SEM 1\\Machine Learning\\Python_projecten\\UFC_Forecaster\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:39:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross-Validated Log Loss: 0.2966\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCalculating Log Loss with 5-Fold Cross-Validation...\")\n",
    "# We use 'neg_log_loss' because sklearn tries to maximize scores\n",
    "neg_log_loss_scores = cross_val_score(model, X_final, y, cv=cv, scoring='neg_log_loss')\n",
    "avg_log_loss = -neg_log_loss_scores.mean()\n",
    "\n",
    "print(f\"Average Cross-Validated Log Loss: {avg_log_loss:.4f}\")"
   ]
  }
 ]
}
